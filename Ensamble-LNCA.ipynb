{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de las librerías de los widgets, numpy para el manejo de vectores y operaciones con arreglos, pandas para la \n",
    "# extracción de tablas y características, io para funciones del sistema y display para mostrar los widgets en nuestro \n",
    "# notebook\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from io import StringIO\n",
    "from IPython.display import display\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from statistics import mode\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delimitador de los valores en el archivo\n",
    "delim = \",\"\n",
    "\n",
    "#Número de filas que se excluyen del set de datos (Observaciones, atributos y clases)\n",
    "rows = 3\n",
    "\n",
    "#Widget para ingresar un valor entero\n",
    "w = widgets.IntSlider(\n",
    "    value=2,\n",
    "    min=2,\n",
    "    max=12,\n",
    "    step=1,\n",
    "    description='#',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "#Ingresar en número de pliegues\n",
    "Folds = widgets.IntSlider(\n",
    "    value=2,\n",
    "    min=2,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='#',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "#Botón de comienzo del ensamble\n",
    "Start = widgets.Button(\n",
    "        description='Comenzar',\n",
    "        disabled=False,\n",
    "        button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Comenzar ensamble',\n",
    "        icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    "        )\n",
    "# #Botón para calcular PCA sobre los datos\n",
    "# calc = widgets.Button(\n",
    "#         description='Calcular PCA',\n",
    "#         disabled=False,\n",
    "#         button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "#         tooltip='Calcular PCA sobre los datos para NAive BAyes y Perceoptrón',\n",
    "#         icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    "#         )\n",
    "\n",
    "#Widget para el botón que inicia la secuencia del PCA\n",
    "b = widgets.Button(\n",
    "    description='Cargar datos',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Cargar datos',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "#Widget de salida\n",
    "output = widgets.Output()\n",
    "\n",
    "\n",
    "#Widget para subir el archivo\n",
    "up= widgets.FileUpload(\n",
    "    accept='',  # Archivos o extenciones válidas. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
    "    multiple=False  # Cambiar a verdadero para aceptar más de un archivo pero no está soportado por la secuencia de lectura\n",
    ")\n",
    "\n",
    "# Convertidor del archivo\n",
    "# Cuando se usa el widget para subir archivos, el formato en el que arroja el documento no puede usarse directamente en la \n",
    "# función read_csv de pandas ya que la estructura es un JSON.\n",
    "\n",
    "# Se tiene que extraer el contenido de una serie de etiquetas, siendo la última (content) la\n",
    "# importante. Esta función sólo extrae de la etiqueta content el archivo leido para luego pasarlo a una cadena o buffer\n",
    "# con formato utf-8 que la función read_csv pueda leer.\n",
    "\n",
    "def content_parser():\n",
    "    # Si el contenido de up no tiene nada, entonces se manda el mensaje de error\n",
    "    \n",
    "    if up.value == {}:\n",
    "        # with output se encarga de mostrar valores en el widget de salida (output)\n",
    "        with output:\n",
    "            print('No CSV loaded')    \n",
    "    else:\n",
    "        # Inicialización de las variables typ y content\n",
    "        typ, content = \"\", \"\"\n",
    "        \n",
    "        # Extracción del contenido del widget up\n",
    "        up_value = up.value\n",
    "        \n",
    "        # Recorre internamente la estructura del JSON que se obtuvo del widget up\n",
    "        for i in up_value.keys():\n",
    "            \n",
    "            # Del valor value de nuestra variable up se extrae el componente type de metadata para verificar que es un\n",
    "            # archivo soportado\n",
    "            \n",
    "            typ = up_value[i][\"metadata\"][\"type\"]\n",
    "            \n",
    "            # Verificación del tipo de archivo\n",
    "            \n",
    "            if typ == \"application/vnd.ms-excel\" or typ == \"text/plain\":\n",
    "                # Extracción del contenido de la etiqueta content\n",
    "                content = up_value[i][\"content\"]\n",
    "                content_str = str(content, 'utf-8')\n",
    "                \n",
    "                if content_str != \"\":\n",
    "                    str_io = StringIO(content_str) \n",
    "                    return str_io\n",
    "\n",
    "def on_b_clicked(b):\n",
    "    # Llamado a la función que permite parsear el contenido\n",
    "    content = content_parser()\n",
    "    \n",
    "    #Si no se regresa un valor parseado entonces no se ejecuta nada de código y sólo se muestra el mensaje de error\n",
    "    if content is not None:\n",
    "        # Lectura del csv desde el buffer de memoria content, quitando los indices de las columnas, estableciendo el\n",
    "        # delimitador del contenido y quitando el header que pueda \n",
    "        data = pd.read_csv(content, index_col=False, sep=delim, header=None)\n",
    "        \n",
    "        # Extracción de las dimensiones de nuestro archivo, para poder determinar el final de las características y de las\n",
    "        # observaciones\n",
    "        xDim, yDim = data.shape\n",
    "        \n",
    "        # Extracción del número de observaciones, atributos y clases de nuestros datos\n",
    "        noObservations = data[0][0]    \n",
    "        noAttributes = data[0][1]\n",
    "        noClasses = data[0][2]\n",
    "        \n",
    "        # Impresión de los valores obtenidos\n",
    "        print(\"Información de los datos cargados: \")\n",
    "        print(\"# de Observaciones {}\".format(noObservations))\n",
    "        print(\"# de Atributos {}\".format(noAttributes))\n",
    "        print(\"# de Clases {}\".format(noClasses))\n",
    "        \n",
    "        w.max = noAttributes\n",
    "        # Extracción de las primeras n columnas que corresponden a las características de los datos\n",
    "        # Se elimina también la columna de los índices para reiniciar el contador y no empezar desde 3\n",
    "        # Se debe tomar en cuenta que la dimensión en Y es mayor que la dimensión total, por ejemplo, si el dataset\n",
    "        # tiene de 0 a 15 características, entonces la dimensión de Y será de 16, por eso se le resta uno a la dimensión Y\n",
    "        global dataSet\n",
    "        dataSet = data.iloc[rows:,:yDim].reset_index(drop=True)\n",
    "        \n",
    "        # Extracción de la columna que contiene a las clases de nuestro data set\n",
    "        # Se sigue el mismo proceso, pero ahora se mantiene sólo la última columna. Se deja entre [] para que el resultado\n",
    "        # se quede en las mismas condiciones que el anterior\n",
    "        global classes\n",
    "        classes = data.iloc[rows:,yDim - 1 ].reset_index(drop=True)\n",
    "        # Impresión del data set (primeros 5 valores) y de las clases\n",
    "        print(dataSet.head())\n",
    "        print(\"\\n\")\n",
    "        print(classes.head())\n",
    "        print(\"\\n\")\n",
    "        display(Start)\n",
    "                \n",
    "#Función para ejecutarse cuando el botón \"startPCA\" se active\n",
    "def on_Start_clicked(Start):\n",
    "    \n",
    "        NoComp = w.value\n",
    "        #dataSet.head()\n",
    "        pca = PCA(n_components=NoComp)\n",
    "        \n",
    "        print(\"Comenzando PCA con los componentes elegidos\\n\")\n",
    "        \n",
    "        global X_std\n",
    "        X_std = StandardScaler().fit_transform(dataSet)\n",
    "\n",
    "        pca.fit(X_std)\n",
    "        global DatosPCA\n",
    "        DatosPCA = pca.transform(X_std)\n",
    "    \n",
    "        print(\"Radio de varianza\")\n",
    "        print(pca.explained_variance_ratio_)\n",
    "        print(\"\\n\")\n",
    "        print(\"Valores singulares\")\n",
    "        print(pca.singular_values_)\n",
    "        print(\"\\n\")\n",
    "        print(\"Matriz de proyección\")\n",
    "        print((pca.components_).T)\n",
    "        print(\"\\n\")\n",
    "        folds = Folds.value\n",
    "        # Naive Bayes\n",
    "        X = DatosPCA\n",
    "        y = classes.values\n",
    "    \n",
    "        #Arreglos de predicciones del ensamble para un elemento\n",
    "        y_pred_Ensamble = []\n",
    "    \n",
    "        #Arreglos de efectividad parciales de los clasificadores\n",
    "        ParcialesBayes = []\n",
    "        ParcialesPercep = []\n",
    "        ParcialesArbol = []\n",
    "        ParcialesEnsamble = []\n",
    "    \n",
    "        #Desea mezclar las muestras?\n",
    "        kf = StratifiedKFold( n_splits = folds , shuffle = True , random_state = None )\n",
    "    \n",
    "    #     for train_index, test_index in kf.split(X, y):\n",
    "    #         #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    #         X_train, X_test = X[train_index], X[test_index]    \n",
    "    #         y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "        #TODO: SEPARAR EL ÁRBOL DE DECISIÓN PARA NO APLICARLE PCA\n",
    "    #    for i,split in enumerate kf.split(X, y):\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "            #print( \"    Comienzo del entrenamiento, vuelta {}\".format(i))\n",
    "            #Bayes\n",
    "            bayes_ingenuo = GaussianNB()\n",
    "            # predicción \n",
    "            y_pred = bayes_ingenuo.fit(X_train, y_train).predict(X_test)\n",
    "            # Matriz de confusión\n",
    "            cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "            ResBayes = 1 - (y_test != y_pred).sum()/y_test.shape[0]\n",
    "            ParcialesBayes.append(ResBayes)\n",
    "            print(\"Aplicando Naive Bayes sobre datos luego del PCA\")\n",
    "            print(\"Cantidad de errores de clasificación sobre un total de {0} casos: {1}\"\n",
    "                    .format(y_test.shape[0],(y_test != y_pred).sum()))\n",
    "            print(\"Efectividad del algoritmo: {0: .2f}\"\n",
    "                    .format(ResBayes))\n",
    "            # Graficando la matriz de confusión\n",
    "            sns.heatmap(cnf_matrix.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "            plt.xlabel('Clase verdadera')\n",
    "            plt.ylabel('Clase predecida')\n",
    "            plt.title('Matriz de Confusión')\n",
    "            plt.show()\n",
    "\n",
    "            #Perceptron\n",
    "            ppn = Perceptron( eta0=0.1, random_state=0)\n",
    "            ppn.fit(X_train, y_train)\n",
    "            y_predA = ppn.predict(X_test)\n",
    "        \n",
    "            cnf_matrix2 = confusion_matrix(y_test, y_predA)\n",
    "            ResPercep = 1 - (y_test != y_predA).sum()/y_test.shape[0]\n",
    "            ParcialesPercep.append(ResPercep)\n",
    "            print(\"Aplicando Perceptrón sobre datos luego del PCA\")\n",
    "            print(\"Número de iteraciones\")\n",
    "            print(ppn.n_iter_)\n",
    "            print(\"Cantidad de errores de clasificación sobre un total de {0} casos: {1}\"\n",
    "                    .format(y_test.shape[0],(y_test != y_predA).sum()))\n",
    "            print(\"Efectividad del algoritmo: {0: .2f}\"\n",
    "                    .format(ResPercep))\n",
    "            # Graficando la matriz de confusión\n",
    "            sns.heatmap(cnf_matrix2.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "            plt.xlabel('Clase verdadera')\n",
    "            plt.ylabel('Clase predecida')\n",
    "            plt.title('Matriz de Confusión')\n",
    "            plt.show()\n",
    "    \n",
    "    \n",
    "            #Arbol de decisión\n",
    "            clf = DecisionTreeClassifier()\n",
    "            clf = clf.fit(X_train ,y_train)\n",
    "            y_predP = clf.predict(X_test)\n",
    "\n",
    "            cnf_matrix3 = confusion_matrix(y_test, y_predP)\n",
    "            ResArbol = 1 - (y_test != y_predP).sum()/y_test.shape[0]\n",
    "            ParcialesArbol.append(ResArbol)\n",
    "            print(\"Aplicando Árbol de decisión sobre datos luego del PCA\")\n",
    "            print(\"Cantidad de errores de clasificación sobre un total de {0} casos: {1}\"\n",
    "                    .format(y_test.shape[0],(y_test != y_predP).sum()))\n",
    "            print(\"Efectividad del algoritmo: {0: .2f}\"\n",
    "                    .format(ResArbol))\n",
    "            # Graficando la matriz de confusión\n",
    "            sns.heatmap(cnf_matrix3.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "            plt.xlabel('Clase verdadera')\n",
    "            plt.ylabel('Clase predecida')\n",
    "            plt.title('Matriz de Confusión')\n",
    "            plt.show()\n",
    "    \n",
    "            #Ensamble sobre resultados parciales\n",
    "    \n",
    "            for i in range(len(y_pred)):\n",
    "                y1 = y_pred[i]\n",
    "                y2 = y_predP[i]\n",
    "                y3 = y_predA[i]\n",
    "                x = [y1, y2, y3]\n",
    "                if y1 != y2 and y1 != y3 and y2 != y3 :\n",
    "                    y_pred_Ensamble.append(random.choice(x))\n",
    "                else :\n",
    "                    y_pred_Ensamble.append(mode(x))\n",
    "\n",
    "            cnf_matrixEnsamble = confusion_matrix(y_test, y_pred_Ensamble)\n",
    "            ResEnsamble = 1 - (y_test != y_pred_Ensamble).sum()/y_test.shape[0]\n",
    "            ParcialesEnsamble.append(ResEnsamble)\n",
    "            print(\"Aplicando el ensamble, resultados parciales\")\n",
    "            print(\"Cantidad de errores de clasificación sobre un total de {0} casos: {1}\"\n",
    "                    .format(y_test.shape[0],(y_test != y_pred_Ensamble).sum()))\n",
    "            print(\"Efectividad del algoritmo: {0: .2f}\"\n",
    "                    .format(ResEnsamble))\n",
    "            # Graficando la matriz de confusión\n",
    "            sns.heatmap(cnf_matrixEnsamble.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "            plt.xlabel('Clase verdadera')\n",
    "            plt.ylabel('Clase predecida')\n",
    "            plt.title('Matriz de Confusión')\n",
    "            plt.show()\n",
    "            y_pred_Ensamble = []\n",
    "\n",
    "        print(\"Resultados de efectividad promedio:\")\n",
    "        avgBay = sum(ParcialesBayes)/len(ParcialesBayes)\n",
    "        avgPer = sum(ParcialesPercep)/len(ParcialesPercep)\n",
    "        avgArb = sum(ParcialesArbol)/len(ParcialesArbol)\n",
    "        avgEns = sum(ParcialesEnsamble)/len(ParcialesEnsamble)\n",
    "\n",
    "        print(\"Naive Bayes: \")\n",
    "        print(avgBay)\n",
    "\n",
    "        print(\"Perceptrón: \")\n",
    "        print(avgPer)\n",
    "\n",
    "        print(\"Árbol de decisión: \")\n",
    "        print(avgArb)\n",
    "\n",
    "        print(\"Ensamble aplicado: \")\n",
    "        print(avgEns)\n",
    "           \n",
    "    \n",
    "    # Asignación de la función on_button_clicked al evento on_click del botón b \n",
    "Start.on_click(on_Start_clicked)\n",
    "b.on_click(on_b_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargue el archivo .txt o .csv que contiene los datos a analizar y presione el botón cuando esté listo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941c7e760bb74879aa0e8befe21170b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={'Slice409.txt': {'metadata': {'name': 'Slice409.txt', 'type': 'text/plain', 'size': 619551, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c0afa27e054dad8344187474cbc99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Cargar datos', icon='check', style=ButtonStyle(), tooltip='Cargar datos')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingrese el número de componentes para el proceso de PCA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69a6641b9954fc1aae6d1e0c5be4a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=49, continuous_update=False, description='#', max=385, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elija el número de pliegues para la validación cruzada\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e976d544a6d64b08b401648b1fdd1f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=3, continuous_update=False, description='#', max=10, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6553c1b99740f3b816a072e3716d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Si los datos son correctos, presione el botón para comenzar \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Cargue el archivo .txt o .csv que contiene los datos a analizar y presione el botón cuando esté listo\")\n",
    "display(up, b)\n",
    "print(\"\\nIngrese el número de componentes para el proceso de PCA\")\n",
    "display(w)\n",
    "print(\"\\nElija el número de pliegues para la validación cruzada\")\n",
    "display(Folds, output)\n",
    "print(\"\\n\\nSi los datos son correctos, presione el botón para comenzar \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = StratifiedKFold( n_splits = 2 , shuffle = True , random_state = None )\n",
    "# for i,split in enumerate (kf.split(X, y)):\n",
    "#     print (i)\n",
    "#     print(\"Dato X = 0\")\n",
    "#     print(split[0])\n",
    "#     print(\"Dato X = 1\")\n",
    "#     print(split[1])\n",
    "    \n",
    "# print(\"Parte dos :D\")\n",
    "# for train_index, test_index in kf.split(X, y):\n",
    "#     print(\"TRAIN:\", train_index, \"\\nTEST:\", test_index)\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #Módulo para ver los datos del PCA antes y después de normalizar con 2 componentes\n",
    "#    if(NoComp == 2):\n",
    "#         Y = pca.transform(X_std)\n",
    "#         y = classes.iloc[:,0].values\n",
    "    \n",
    "#         with plt.style.context('seaborn-whitegrid'):\n",
    "#             plt.figure(figsize=(6, 4))\n",
    "#             for lab, col in zip((0, 1, 2, 3, 4, 5, 6),\n",
    "#                         ('magenta', 'cyan', 'limegreen', 'blue', 'red', 'pink', 'black')):\n",
    "#                 plt.scatter(Y[y==lab, 0],\n",
    "#                         -Y[y==lab, 1],\n",
    "#                         label=lab,\n",
    "#                         c=col)\n",
    "#         plt.title(\"Datos normalizados\")\n",
    "#         plt.xlabel('Componente Principal 1')\n",
    "#         plt.ylabel('Componente Principal 2')\n",
    "#         plt.legend(loc='best')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "    \n",
    "#         pca.fit(dataSet)\n",
    "\n",
    "\n",
    "#         print(\"Radio de varianza\")\n",
    "#         print(pca.explained_variance_)\n",
    "#         print(\"Valores singulares\")\n",
    "#         print(pca.singular_values_)\n",
    "\n",
    "#         Y = pca.transform(dataSet)\n",
    "#         y = classes.iloc[:,0].values\n",
    "    \n",
    "#         #(pca.components_).T\n",
    "\n",
    "#         with plt.style.context('seaborn-whitegrid'):\n",
    "#             plt.figure(figsize=(6, 4))\n",
    "#             for lab, col in zip((0, 1, 2, 3, 4, 5),\n",
    "#                         ('magenta', 'cyan', 'limegreen', 'blue', 'red', 'pink')):\n",
    "#                 plt.scatter(Y[y==lab, 0],\n",
    "#                             -Y[y==lab, 1],\n",
    "#                             label=lab,\n",
    "#                             c=col)\n",
    "            \n",
    "#         plt.title(\"Datos no normalizados\")\n",
    "#         plt.xlabel('Componente Principal 1')\n",
    "#         plt.ylabel('Componente Principal 2')\n",
    "#         plt.legend(loc='best')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# datas = pd.read_csv(\"PruebaPy.csv\", skiprows=3, header=None)\n",
    "# xDim, yDim = datas.shape\n",
    "# XD = datas.iloc[:,0:yDim-2].values\n",
    "# YD = datas.iloc[:,yDim-1].values\n",
    "\n",
    "# print(XD)\n",
    "# print(YD)\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# X_stdP = StandardScaler().fit_transform(XD)\n",
    "# pca = PCA(n_components=8)\n",
    "# pca.fit(X_stdP)\n",
    "\n",
    "# XDP = pca.transform(X_stdP)\n",
    "# print(XDP)\n",
    "\n",
    "\n",
    "# #NewCode\n",
    "# import seaborn as sns\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.linear_model import Perceptron\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# #NewCode 2\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# # Naive Bayes\n",
    "# X = XDP\n",
    "# y = YD\n",
    "\n",
    "# y_pred_Ensamble = []\n",
    "# y_pred_Ensamble2 = []\n",
    "# y_pred_Ensamble3 = []\n",
    "# ParcialesBayes = []\n",
    "# ParcialesPercep = []\n",
    "# ParcialesArbol = []\n",
    "# ParcialesEnsamble = []\n",
    "\n",
    "# from statistics import mode\n",
    "\n",
    "# kf = StratifiedKFold( n_splits = 5 , shuffle = True , random_state = None )\n",
    "# for train_index, test_index in kf.split(X, y):\n",
    "#     #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "#     print( \"    Comienzo del entrenamiento   \")\n",
    "#     #Bayes\n",
    "#     bayes_ingenuo = GaussianNB()\n",
    "#     # predicción \n",
    "#     y_pred = bayes_ingenuo.fit(X_train, y_train).predict(X_test)\n",
    "#     # Matriz de confusión\n",
    "#     cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "#     ResBayes = 1 - (y_test != y_pred).sum()/y_test.shape[0]\n",
    "#     ParcialesBayes.append(ResBayes)\n",
    "#     print(\"Aplicando Naive Bayes sobre datos luego del PCA\")\n",
    "#     print(\"Cantidad de errores de clasificación sobre un total de {0} casos: {1}\"\n",
    "#             .format(y_test.shape[0],(y_test != y_pred).sum()))\n",
    "#     print(\"Efectividad del algoritmo: {0: .2f}\"\n",
    "#             .format(ResBayes))\n",
    "#     # Graficando la matriz de confusión\n",
    "#     sns.heatmap(cnf_matrix.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "#     plt.xlabel('Clase verdadera')\n",
    "#     plt.ylabel('Clase predecida')\n",
    "#     plt.title('Matriz de Confusión')\n",
    "#     plt.show()\n",
    "    \n",
    "# #     sns.stripplot( x = y_test, y = y_pred, jitter = 0.9)\n",
    "# #     plt.show()\n",
    "    \n",
    "#     #Perceptron\n",
    "#     ppn = Perceptron( eta0=0.1, random_state=0)\n",
    "#     ppn.fit(X_train, y_train)\n",
    "#     y_predA = ppn.predict(X_test)\n",
    "    \n",
    "#     cnf_matrix2 = confusion_matrix(y_test, y_predA)\n",
    "#     ResPercep = 1 - (y_test != y_predA).sum()/y_test.shape[0]\n",
    "#     ParcialesPercep.append(ResPercep)\n",
    "#     print(\"Aplicando Perceptrón sobre datos luego del PCA\")\n",
    "#     print(\"Número de iteraciones\")\n",
    "#     print(ppn.n_iter_)\n",
    "#     print(\"Cantidad de errores de clasificación sobre un total de {0} casos: {1}\"\n",
    "#             .format(y_test.shape[0],(y_test != y_predA).sum()))\n",
    "#     print(\"Efectividad del algoritmo: {0: .2f}\"\n",
    "#             .format(ResPercep))\n",
    "#     # Graficando la matriz de confusión\n",
    "#     sns.heatmap(cnf_matrix2.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "#     plt.xlabel('Clase verdadera')\n",
    "#     plt.ylabel('Clase predecida')\n",
    "#     plt.title('Matriz de Confusión')\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "#     #Arbol de decisión\n",
    "#     clf = DecisionTreeClassifier()\n",
    "#     clf = clf.fit(X_train ,y_train)\n",
    "#     y_predP = clf.predict(X_test)\n",
    "\n",
    "#     cnf_matrix3 = confusion_matrix(y_test, y_predP)\n",
    "#     ResArbol = 1 - (y_test != y_predP).sum()/y_test.shape[0]\n",
    "#     ParcialesArbol.append(ResArbol)\n",
    "#     print(\"Aplicando Árbol de decisión sobre datos luego del PCA\")\n",
    "#     print(\"Cantidad de errores de clasificación sobre un total de {0} casos: {1}\"\n",
    "#             .format(y_test.shape[0],(y_test != y_predP).sum()))\n",
    "#     print(\"Efectividad del algoritmo: {0: .2f}\"\n",
    "#             .format(ResArbol))\n",
    "#     # Graficando la matriz de confusión\n",
    "#     sns.heatmap(cnf_matrix3.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "#     plt.xlabel('Clase verdadera')\n",
    "#     plt.ylabel('Clase predecida')\n",
    "#     plt.title('Matriz de Confusión')\n",
    "#     plt.show()\n",
    "    \n",
    "#     #Ensamble sobre resultados parciales\n",
    "    \n",
    "#     for i in range(len(y_pred)):\n",
    "#         y1 = y_pred[i]\n",
    "#         y2 = y_predP[i]\n",
    "#         y3 = y_predA[i]\n",
    "    \n",
    "# #     if y1 != y2 and y1 != y3 and y2 != y3:\n",
    "# #         p1 = 1 - (y_test != y_pred).sum()/y_test.shape[0]\n",
    "# #         p2 = 1 - (y_test != y_predA).sum()/y_test.shape[0]\n",
    "# #         p3 = 1 - (y_test != y_predP).sum()/y_test.shape[0]\n",
    "        \n",
    "# #     else:\n",
    "#         x = [y1, y2, y3]\n",
    "#         y_pred_Ensamble.append(mode(x))\n",
    "\n",
    "#     cnf_matrixEnsamble = confusion_matrix(y_test, y_pred_Ensamble)\n",
    "#     ResEnsamble = 1 - (y_test != y_pred_Ensamble).sum()/y_test.shape[0]\n",
    "#     ParcialesEnsamble.append(ResEnsamble)\n",
    "#     print(\"Aplicando el ensamble, resultados parciales\")\n",
    "#     print(\"Cantidad de errores de clasificación sobre un total de {0} casos: {1}\"\n",
    "#             .format(y_test.shape[0],(y_test != y_pred_Ensamble).sum()))\n",
    "#     print(\"Efectividad del algoritmo: {0: .2f}\"\n",
    "#             .format(ResEnsamble))\n",
    "#     # Graficando la matriz de confusión\n",
    "#     sns.heatmap(cnf_matrixEnsamble.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "#     plt.xlabel('Clase verdadera')\n",
    "#     plt.ylabel('Clase predecida')\n",
    "#     plt.title('Matriz de Confusión')\n",
    "#     plt.show()\n",
    "#     y_pred_Ensamble = []\n",
    "\n",
    "# print(\"Resultados de efectividad promedio:\")\n",
    "# avgBay = sum(ParcialesBayes)/len(ParcialesBayes)\n",
    "# avgPer = sum(ParcialesPercep)/len(ParcialesPercep)\n",
    "# avgArb = sum(ParcialesArbol)/len(ParcialesArbol)\n",
    "# avgEns = sum(ParcialesEnsamble)/len(ParcialesEnsamble)\n",
    "\n",
    "# print(\"Naive Bayes: \")\n",
    "# print(avgBay)\n",
    "\n",
    "# print(\"Perceptrón: \")\n",
    "# print(avgPer)\n",
    "\n",
    "# print(\"Árbol de decisión: \")\n",
    "# print(avgArb)\n",
    "\n",
    "# print(\"Ensamble aplicado: \")\n",
    "# print(avgEns)\n",
    "\n",
    "# #     #Generar un árbol de decisión\n",
    "# # from sklearn.tree import export_graphviz\n",
    "# # from sklearn.externals.six import StringIO  \n",
    "# # from IPython.display import Image  \n",
    "# # import pydotplus\n",
    "\n",
    "# # dot_data = StringIO()\n",
    "# # export_graphviz(clf, out_file=dot_data,  \n",
    "# #                 filled=True, rounded=True,\n",
    "# #                 special_characters=True, class_names=['0','1','2','3','4','5','6'])\n",
    "# # graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# # graph.write_png(\"ArbolWPCA.png\")\n",
    "# # Image(graph.create_png())\n",
    "\n",
    "\n",
    "\n",
    "# # plt.scatter(y_test, y_pred_Ensamble, jitter = True)\n",
    "# # plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
